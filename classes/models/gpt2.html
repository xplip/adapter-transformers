

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>OpenAI GPT2 &mdash; adapter-transformers  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="MBart" href="mbart.html" />
    <link rel="prev" title="DistilBERT" href="distilbert.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> adapter-transformers
          

          
            
            <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">adapter-transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../adapters.html">Introduction to Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../adapter_composition.html">Adapter Activation and Composition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prediction_heads.html">Prediction Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../extending.html">Extending the Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../v2_transition.html">Transitioning from v1 to v2</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Hub</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../loading.html">Loading Pre-Trained Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing to Adapter Hub</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Related Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../adapter_config.html">Adapter Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_adapters_config.html">Model Adapters Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_modules.html">Adapter Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_layer.html">AdapterLayerBaseMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_mixins.html">Model Mixins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_utils.html">Adapter Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../weights_loaders.html">Weights Loaders</a></li>
</ul>
<p class="caption"><span class="caption-text">Supported Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">OpenAI GPT2</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2config">GPT2Config</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2tokenizer">GPT2Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2tokenizerfast">GPT2TokenizerFast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2-specific-outputs">GPT2 specific outputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2model">GPT2Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2modelwithheads">GPT2ModelWithHeads</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2lmheadmodel">GPT2LMHeadModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2doubleheadsmodel">GPT2DoubleHeadsModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gpt2forsequenceclassification">GPT2ForSequenceClassification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">adapter-transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>OpenAI GPT2</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/classes/models/gpt2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="openai-gpt2">
<h1>OpenAI GPT2<a class="headerlink" href="#openai-gpt2" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>OpenAI GPT-2 model was proposed in <a class="reference external" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> by Alec
Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever. It’s a causal (unidirectional)
transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.</p>
<p>The abstract from the paper is the following:</p>
<p><em>GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million
web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some
text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks
across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than
10X the amount of data.</em></p>
<p>Tips:</p>
<ul class="simple">
<li><p>GPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than
the left.</p></li>
<li><p>GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next
token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be
observed in the <cite>run_generation.py</cite> example script.</p></li>
<li><p>The PyTorch models can take the <cite>past</cite> as input, which is the previously computed key/value attention pairs. Using
this <cite>past</cite> value prevents the model from re-computing pre-computed values in the context of text generation. See
<a class="reference external" href="../quickstart.html#using-the-past">reusing the past in generative models</a> for more information on the usage of
this argument.</p></li>
</ul>
<p><a class="reference external" href="https://transformer.huggingface.co/doc/gpt2-large">Write With Transformer</a> is a webapp created and hosted by
Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five
different sizes: small, medium, large, xl and a distilled version of the small checkpoint: <cite>distilgpt-2</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class is nearly identical to the PyTorch implementation of BERT in Huggingface Transformers.
For more information, visit <a class="reference external" href="https://huggingface.co/transformers/model_doc/bert.html">the corresponding section in their documentation</a>.</p>
</div>
</div>
<div class="section" id="gpt2config">
<h2>GPT2Config<a class="headerlink" href="#gpt2config" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.GPT2Config">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GPT2Config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_size</span><span class="o">=</span><span class="default_value">50257</span></em>, <em class="sig-param"><span class="n">n_positions</span><span class="o">=</span><span class="default_value">1024</span></em>, <em class="sig-param"><span class="n">n_ctx</span><span class="o">=</span><span class="default_value">1024</span></em>, <em class="sig-param"><span class="n">n_embd</span><span class="o">=</span><span class="default_value">768</span></em>, <em class="sig-param"><span class="n">n_layer</span><span class="o">=</span><span class="default_value">12</span></em>, <em class="sig-param"><span class="n">n_head</span><span class="o">=</span><span class="default_value">12</span></em>, <em class="sig-param"><span class="n">n_inner</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">activation_function</span><span class="o">=</span><span class="default_value">'gelu_new'</span></em>, <em class="sig-param"><span class="n">resid_pdrop</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">embd_pdrop</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">attn_pdrop</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="default_value">1e-05</span></em>, <em class="sig-param"><span class="n">initializer_range</span><span class="o">=</span><span class="default_value">0.02</span></em>, <em class="sig-param"><span class="n">summary_type</span><span class="o">=</span><span class="default_value">'cls_index'</span></em>, <em class="sig-param"><span class="n">summary_use_proj</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">summary_activation</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">summary_proj_to_labels</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">summary_first_dropout</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">gradient_checkpointing</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">bos_token_id</span><span class="o">=</span><span class="default_value">50256</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="o">=</span><span class="default_value">50256</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2Config" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the configuration class to store the configuration of a <a class="reference internal" href="#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> or a
<code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2Model</span></code>. It is used to instantiate a GPT-2 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the GPT-2 <a class="reference external" href="https://huggingface.co/gpt2">small</a> architecture.</p>
<p>Configuration objects inherit from <code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code> and can be used to control the model
outputs. Read the documentation from <code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code> for more information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 50257) – Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_ids</span></code> passed when calling <a class="reference internal" href="#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> or
<code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2Model</span></code>.</p></li>
<li><p><strong>n_positions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1024) – The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).</p></li>
<li><p><strong>n_ctx</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1024) – Dimensionality of the causal mask (usually same as n_positions).</p></li>
<li><p><strong>n_embd</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 768) – Dimensionality of the embeddings and hidden states.</p></li>
<li><p><strong>n_layer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) – Number of hidden layers in the Transformer encoder.</p></li>
<li><p><strong>n_head</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) – Number of attention heads for each attention layer in the Transformer encoder.</p></li>
<li><p><strong>n_inner</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to None) – Dimensionality of the inner feed-forward layers. <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> will set it to 4 times n_embd</p></li>
<li><p><strong>activation_function</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;gelu&quot;</span></code>) – Activation function, to be selected in the list <code class="xref py py-obj docutils literal notranslate"><span class="pre">[&quot;relu&quot;,</span> <span class="pre">&quot;silu&quot;,</span> <span class="pre">&quot;gelu&quot;,</span> <span class="pre">&quot;tanh&quot;,</span> <span class="pre">&quot;gelu_new&quot;]</span></code>.</p></li>
<li><p><strong>resid_pdrop</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) – The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p></li>
<li><p><strong>embd_pdrop</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0.1) – The dropout ratio for the embeddings.</p></li>
<li><p><strong>attn_pdrop</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) – The dropout ratio for the attention.</p></li>
<li><p><strong>layer_norm_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-5) – The epsilon to use in the layer normalization layers</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.02) – The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p></li>
<li><p><strong>summary_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;cls_index&quot;</span></code>) – <p>Argument used when doing sequence summary, used in the models <a class="reference internal" href="#transformers.GPT2DoubleHeadsModel" title="transformers.GPT2DoubleHeadsModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2DoubleHeadsModel</span></code></a>
and <code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2DoubleHeadsModel</span></code>.</p>
<p>Has to be one of the following options:</p>
<blockquote>
<div><ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;last&quot;</span></code>: Take the last token hidden state (like XLNet).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;first&quot;</span></code>: Take the first token hidden state (like BERT).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>: Take the mean of all tokens hidden states.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;cls_index&quot;</span></code>: Supply a Tensor of classification token position (like GPT/GPT-2).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;attn&quot;</span></code>: Not implemented now, use multi-head attention.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>summary_use_proj</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – <p>Argument used when doing sequence summary, used in the models <a class="reference internal" href="#transformers.GPT2DoubleHeadsModel" title="transformers.GPT2DoubleHeadsModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2DoubleHeadsModel</span></code></a>
and <code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2DoubleHeadsModel</span></code>.</p>
<p>Whether or not to add a projection after the vector extraction.</p>
</p></li>
<li><p><strong>summary_activation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – <p>Argument used when doing sequence summary. Used in for the multiple choice head in
<a class="reference internal" href="#transformers.GPT2DoubleHeadsModel" title="transformers.GPT2DoubleHeadsModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2DoubleHeadsModel</span></code></a>.</p>
<p>Pass <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;tanh&quot;</span></code> for a tanh activation to the output, any other value will result in no activation.</p>
</p></li>
<li><p><strong>summary_proj_to_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – <p>Argument used when doing sequence summary, used in the models <a class="reference internal" href="#transformers.GPT2DoubleHeadsModel" title="transformers.GPT2DoubleHeadsModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2DoubleHeadsModel</span></code></a>
and <code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2DoubleHeadsModel</span></code>.</p>
<p>Whether the projection outputs should have <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.hidden_size</span></code> classes.</p>
</p></li>
<li><p><strong>summary_first_dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) – <p>Argument used when doing sequence summary, used in the models <a class="reference internal" href="#transformers.GPT2DoubleHeadsModel" title="transformers.GPT2DoubleHeadsModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2DoubleHeadsModel</span></code></a>
and <code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2DoubleHeadsModel</span></code>.</p>
<p>The dropout ratio to be used after the projection and activation.</p>
</p></li>
<li><p><strong>gradient_checkpointing</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.</p></li>
<li><p><strong>use_cache</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether or not the model should return the last key/values attentions (not used by all models).</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Model</span><span class="p">,</span> <span class="n">GPT2Config</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing a GPT2 configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration</span> <span class="o">=</span> <span class="n">GPT2Config</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing a model from the configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Accessing the model configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="gpt2tokenizer">
<h2>GPT2Tokenizer<a class="headerlink" href="#gpt2tokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.GPT2Tokenizer">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GPT2Tokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">merges_file</span></em>, <em class="sig-param"><span class="n">errors</span><span class="o">=</span><span class="default_value">'replace'</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'&lt;|endoftext|&gt;'</span></em>, <em class="sig-param"><span class="n">bos_token</span><span class="o">=</span><span class="default_value">'&lt;|endoftext|&gt;'</span></em>, <em class="sig-param"><span class="n">eos_token</span><span class="o">=</span><span class="default_value">'&lt;|endoftext|&gt;'</span></em>, <em class="sig-param"><span class="n">add_prefix_space</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2Tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.</p>
<p>This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will
be encoded differently whether it is at the beginning of the sentence (without space) or not:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
<span class="go">[15496, 995]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot; Hello world&quot;</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
<span class="go">[18435, 995]</span>
</pre></div>
</div>
<p>You can get around that behavior by passing <code class="docutils literal notranslate"><span class="pre">add_prefix_space=True</span></code> when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When used with <code class="docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code>, this tokenizer will add a space before each word (even the first
one).</p>
</div>
<p>This tokenizer inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code> which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – Path to the vocabulary file.</p></li>
<li><p><strong>merges_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – Path to the merges file.</p></li>
<li><p><strong>errors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;replace&quot;</span></code>) – Paradigm to follow when decoding bytes to UTF-8. See <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes.decode">bytes.decode</a> for more information.</p></li>
<li><p><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code>) – The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</p></li>
<li><p><strong>bos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code>) – The beginning of sequence token.</p></li>
<li><p><strong>eos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code>) – The end of sequence token.</p></li>
<li><p><strong>add_prefix_space</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to add an initial space to the input. This allows to treat the leading word just as any
other word. (GPT2 tokenizer detect beginning of words by the preceding space).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.GPT2Tokenizer.save_vocabulary">
<code class="sig-name descname">save_vocabulary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>str<span class="p">]</span><a class="headerlink" href="#transformers.GPT2Tokenizer.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method won’t save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The directory in which to save the vocabulary.</p></li>
<li><p><strong>filename_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – An optional prefix to add to the named of the saved files.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Paths to the files saved.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="gpt2tokenizerfast">
<h2>GPT2TokenizerFast<a class="headerlink" href="#gpt2tokenizerfast" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.GPT2TokenizerFast">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GPT2TokenizerFast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">merges_file</span></em>, <em class="sig-param"><span class="n">tokenizer_file</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'&lt;|endoftext|&gt;'</span></em>, <em class="sig-param"><span class="n">bos_token</span><span class="o">=</span><span class="default_value">'&lt;|endoftext|&gt;'</span></em>, <em class="sig-param"><span class="n">eos_token</span><span class="o">=</span><span class="default_value">'&lt;|endoftext|&gt;'</span></em>, <em class="sig-param"><span class="n">add_prefix_space</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2TokenizerFast" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a “fast” GPT-2 tokenizer (backed by HuggingFace’s <cite>tokenizers</cite> library). Based on byte-level
Byte-Pair-Encoding.</p>
<p>This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will
be encoded differently whether it is at the beginning of the sentence (without space) or not:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2TokenizerFast</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2TokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
<span class="go">[15496, 995]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot; Hello world&quot;</span><span class="p">)[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
<span class="go">[18435, 995]</span>
</pre></div>
</div>
<p>You can get around that behavior by passing <code class="docutils literal notranslate"><span class="pre">add_prefix_space=True</span></code> when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When used with <code class="docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code>, this tokenizer needs to be instantiated with
<code class="docutils literal notranslate"><span class="pre">add_prefix_space=True</span></code>.</p>
</div>
<p>This tokenizer inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code> which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – Path to the vocabulary file.</p></li>
<li><p><strong>merges_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – Path to the merges file.</p></li>
<li><p><strong>errors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;replace&quot;</span></code>) – Paradigm to follow when decoding bytes to UTF-8. See <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes.decode">bytes.decode</a> for more information.</p></li>
<li><p><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code>) – The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</p></li>
<li><p><strong>bos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code>) – The beginning of sequence token.</p></li>
<li><p><strong>eos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code>) – The end of sequence token.</p></li>
<li><p><strong>add_prefix_space</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to add an initial space to the input. This allows to treat the leading word just as any
other word. (GPT2 tokenizer detect beginning of words by the preceding space).</p></li>
<li><p><strong>trim_offsets</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether or not the post-processing step should trim offsets to avoid including whitespaces.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.GPT2TokenizerFast.save_vocabulary">
<code class="sig-name descname">save_vocabulary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>str<span class="p">]</span><a class="headerlink" href="#transformers.GPT2TokenizerFast.save_vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method won’t save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The directory in which to save the vocabulary.</p></li>
<li><p><strong>filename_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – An optional prefix to add to the named of the saved files.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Paths to the files saved.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="transformers.GPT2TokenizerFast.slow_tokenizer_class">
<code class="sig-name descname">slow_tokenizer_class</code><a class="headerlink" href="#transformers.GPT2TokenizerFast.slow_tokenizer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer</span></code></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="gpt2-specific-outputs">
<h2>GPT2 specific outputs<a class="headerlink" href="#gpt2-specific-outputs" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput">
<em class="property">class </em><code class="sig-prename descclassname">transformers.models.gpt2.modeling_gpt2.</code><code class="sig-name descname">GPT2DoubleHeadsModelOutput</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mc_loss</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">logits</span><span class="p">:</span> <span class="n">torch.FloatTensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mc_logits</span><span class="p">:</span> <span class="n">torch.FloatTensor</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">past_key_values</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for outputs of models predicting if two sentences are consecutive or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">labels</span></code> is provided) – Language modeling loss.</p></li>
<li><p><strong>mc_loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">mc_labels</span></code> is provided) – Multiple choice classification loss.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_choices,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p></li>
<li><p><strong>mc_logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_choices)</span></code>) – Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).</p></li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Tuple[torch.Tensor]]</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">use_cache=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.use_cache=True</span></code>) – <p>Tuple of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>, containing tuples of tensors of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">embed_size_per_head)</span></code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> input) to speed up sequential decoding.</p>
</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – <p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</p></li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – <p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="gpt2model">
<h2>GPT2Model<a class="headerlink" href="#gpt2model" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.GPT2Model">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GPT2Model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2Model" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare GPT2 Model transformer outputting raw hidden-states without any specific head on top.</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.GPT2Model.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">past_key_values</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2Model.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_ids_length)</span></code>) – <p><code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids_length</span></code> = <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> else
<code class="docutils literal notranslate"><span class="pre">past_key_values[0][0].shape[-2]</span></code> (<code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used, only <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> that do not have their past calculated should be
passed as <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.GPT2Tokenizer" title="transformers.GPT2Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code></a>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code> for
details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Tuple[torch.Tensor]]</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>) – Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> output below). Can be used to speed up sequential decoding. The <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> which
have their past given to this model should not be passed as <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> as they have already been
computed.</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_ids_length)</span></code>, <cite>optional</cite>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul>
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – <p>Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used, optionally only the last <code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_embeds</span></code> have to be input (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code>).</p>
</p></li>
<li><p><strong>use_cache</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – If set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> key value states are returned and can be used to speed up
decoding (see <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code>).</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code> instead of a plain tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModelOutputWithPastAndCrossAttentions</span></code> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>last_hidden_state</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>) – Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used only the last hidden-state of the sequences of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span>
<span class="pre">1,</span> <span class="pre">hidden_size)</span></code> is output.</p>
</li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tuple(torch.FloatTensor))</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">use_cache=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.use_cache=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>, with each tuple having 2 tensors
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">embed_size_per_head)</span></code>) and optionally if
<code class="docutils literal notranslate"><span class="pre">config.is_encoder_decoder=True</span></code> 2 additional tensors of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">encoder_sequence_length,</span> <span class="pre">embed_size_per_head)</span></code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code class="docutils literal notranslate"><span class="pre">config.is_encoder_decoder=True</span></code> in the cross-attention blocks) that can be used (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> input) to speed up sequential decoding.</p>
</li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li><p><strong>cross_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> and <code class="docutils literal notranslate"><span class="pre">config.add_cross_attention=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights of the decoder’s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModelOutputWithPastAndCrossAttentions</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2Model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="gpt2modelwithheads">
<h2>GPT2ModelWithHeads<a class="headerlink" href="#gpt2modelwithheads" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.GPT2ModelWithHeads">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GPT2ModelWithHeads</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads" title="Permalink to this definition">¶</a></dt>
<dd><p>The GPT2 Model that allows the loading of different heads dor different tasks. This enables a flexible use of the
models and adpters.</p>
<blockquote>
<div><p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="simple">
<dt>config (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>): Model configuration class with all the parameters of the model.</dt><dd><p>Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.active_head">
<em class="property">property </em><code class="sig-name descname">active_head</code><a class="headerlink" href="#transformers.GPT2ModelWithHeads.active_head" title="Permalink to this definition">¶</a></dt>
<dd><p>The active prediction head configuration of this model. Can be either the name of a single available head
(string) or a list of multiple available heads. In case of a list of heads, the same base model is forwarded
through all specified heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A string or a list of strings describing the active head configuration.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Union[str, List[str]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.add_adapter">
<code class="sig-name descname">add_adapter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adapter_name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">config</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.add_adapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a new adapter module of the specified type to the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_name</strong> (<em>str</em>) – The name of the adapter module to be added.</p></li>
<li><p><strong>config</strong> (<em>str</em><em> or </em><em>dict</em><em>, </em><em>optional</em>) – <p>The adapter configuration, can be either:</p>
<ul>
<li><p>the string identifier of a pre-defined configuration dictionary</p></li>
<li><p>a configuration dictionary specifying the full config</p></li>
<li><p>if not given, the default configuration for this adapter type will be used</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.add_classification_head">
<code class="sig-name descname">add_classification_head</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">head_name</span></em>, <em class="sig-param"><span class="n">num_labels</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">layers</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">activation_function</span><span class="o">=</span><span class="default_value">'tanh'</span></em>, <em class="sig-param"><span class="n">overwrite_ok</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">multilabel</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">id2label</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.add_classification_head" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a sequence classification head on top of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> (<em>str</em>) – The name of the head.</p></li>
<li><p><strong>num_labels</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of classification labels. Defaults to 2.</p></li>
<li><p><strong>layers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of layers. Defaults to 2.</p></li>
<li><p><strong>activation_function</strong> (<em>str</em><em>, </em><em>optional</em>) – Activation function. Defaults to ‘tanh’.</p></li>
<li><p><strong>overwrite_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – Force overwrite if a head with the same name exists. Defaults to False.</p></li>
<li><p><strong>multilabel</strong> (<em>bool</em><em>, </em><em>optional</em>) – Enable multilabel classification setup. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.add_fusion">
<code class="sig-name descname">add_fusion</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>transformers.adapters.composition.Fuse<span class="p">, </span>list<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">adapter_fusion_config</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">override_kwargs</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.add_fusion" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds AdapterFusion to the model with alll the necessary configurations and weight initializations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_names</strong> – a list of adapter names which should be fused</p></li>
<li><p><strong>adapter_fusion_config</strong> (<em>str</em><em> or </em><em>dict</em>) – <p>adapter fusion configuration, can be either:</p>
<ul>
<li><p>a string identifying a pre-defined adapter fusion configuration</p></li>
<li><p>a dictionary representing the adapter fusion configuration</p></li>
<li><p>the path to a file containing the adapter fusion configuration</p></li>
</ul>
</p></li>
<li><p><strong>override_kwargs</strong> – dictionary items for values which should be overwritten in the default AdapterFusion configuration</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.freeze_model">
<code class="sig-name descname">freeze_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">freeze</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.freeze_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Freezes all weights of the model.</p>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.get_labels">
<code class="sig-name descname">get_labels</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">head_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.get_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the labels the given head is assigning/predictin</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> – (str, optional) the name of the head which labels should be returned. Default is None.</p></li>
<li><p><strong>the name is None the labels of the active head are returned</strong> (<em>If</em>) – </p></li>
</ul>
</dd>
</dl>
<p>Returns: labels</p>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.get_labels_dict">
<code class="sig-name descname">get_labels_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">head_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.get_labels_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the id2label dict for the given hea</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_name</strong> – (str, optional) the name of the head which labels should be returned. Default is None.</p></li>
<li><p><strong>the name is None the labels of the active head are returned</strong> (<em>If</em>) – </p></li>
</ul>
</dd>
</dl>
<p>Returns: id2label</p>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.load_adapter">
<code class="sig-name descname">load_adapter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adapter_name_or_path</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>dict<span class="p">, </span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">version</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">model_name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">load_as</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">with_head</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">custom_weights_loaders</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>transformers.adapters.loading.WeightsLoader<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> &#x2192; str<a class="headerlink" href="#transformers.GPT2ModelWithHeads.load_adapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a pre-trained pytorch adapter module from the local file system or a remote location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_name_or_path</strong> (<em>str</em>) – <p>can be either:</p>
<ul>
<li><p>the identifier of a pre-trained task adapter to be loaded from Adapter Hub</p></li>
<li><p>a path to a directory containing adapter weights saved using <cite>model.saved_adapter()</cite></p></li>
<li><p>a URL pointing to a zip folder containing a saved adapter module</p></li>
</ul>
</p></li>
<li><p><strong>config</strong> (<em>dict</em><em> or </em><em>str</em><em>, </em><em>optional</em>) – The requested configuration of the adapter.
If not specified, will be either: - the default adapter config for the requested adapter if specified -
the global default adapter config</p></li>
<li><p><strong>version</strong> (<em>str</em><em>, </em><em>optional</em>) – The version of the adapter to be loaded.</p></li>
<li><p><strong>model_name</strong> (<em>str</em><em>, </em><em>optional</em>) – The string identifier of the pre-trained model.</p></li>
<li><p><strong>load_as</strong> (<em>str</em><em>, </em><em>optional</em>) – Load the adapter using this name. By default, the name with which the adapter was
saved will be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The name with which the adapter was added to the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.load_adapter_fusion">
<code class="sig-name descname">load_adapter_fusion</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adapter_fusion_name_or_path</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">load_as</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">custom_weights_loaders</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>transformers.adapters.loading.WeightsLoader<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> &#x2192; str<a class="headerlink" href="#transformers.GPT2ModelWithHeads.load_adapter_fusion" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a pre-trained pytorch adapter module from the local file system or a remote location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>adapter_fusion_name_or_path</strong> (<em>str</em>) – <p>can be either:</p>
<ul>
<li><p>the identifier of a pre-trained task adapter fusion module to be loaded from Adapter Hub</p></li>
<li><p>a path to a directory containing adapter weights saved using <cite>model.saved_adapter()</cite></p></li>
<li><p>a URL pointing to a zip folder containing a saved adapter module</p></li>
</ul>
</p></li>
<li><p><strong>config</strong> (<em>dict</em><em> or </em><em>str</em><em>, </em><em>optional</em>) – The requested configuration of the adapter fusion.
If not specified, will be either: - the default adapter config for the requested adapter fusion if
specified - the global default adapter fusion config</p></li>
<li><p><strong>model_name</strong> (<em>str</em><em>, </em><em>optional</em>) – The string identifier of the pre-trained model.</p></li>
<li><p><strong>load_as</strong> (<em>str</em><em>, </em><em>optional</em>) – Load the adapter using this name. By default, the name with which the adapter was
saved will be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The name with which the adapter was added to the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.pre_transformer_forward">
<code class="sig-name descname">pre_transformer_forward</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.pre_transformer_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should be called by every adapter-implementing model at the very beginning of the forward() method.</p>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.save_adapter">
<code class="sig-name descname">save_adapter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">adapter_name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">with_head</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">meta_dict</span><span class="p">:</span> <span class="n">dict</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">custom_weights_loaders</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>transformers.adapters.loading.WeightsLoader<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.save_adapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves an adapter and its configuration file to a directory so that it can be shared or reloaded using
<cite>load_adapter()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the adapter should be saved.</p></li>
<li><p><strong>adapter_name</strong> (<em>str</em>) – Name of the adapter to be saved.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the given adapter name is invalid.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.save_adapter_fusion">
<code class="sig-name descname">save_adapter_fusion</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">adapter_names</span><span class="p">:</span> <span class="n">list</span></em>, <em class="sig-param"><span class="n">custom_weights_loaders</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>transformers.adapters.loading.WeightsLoader<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.save_adapter_fusion" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves an adapter and its configuration file to a directory so that it can be shared or reloaded using
<cite>load_adapter()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the adapter should be saved.</p></li>
<li><p><strong>adapter_name</strong> (<em>str</em>) – Name of the adapter to be saved.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the given adapter name is invalid.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.save_all_adapter_fusions">
<code class="sig-name descname">save_all_adapter_fusions</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">meta_dict</span><span class="p">:</span> <span class="n">dict</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">custom_weights_loaders</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>transformers.adapters.loading.WeightsLoader<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.save_all_adapter_fusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves all adapters of this model together with their configuration to subfolders of the given location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the adapters should be saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.save_all_adapters">
<code class="sig-name descname">save_all_adapters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">with_head</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">meta_dict</span><span class="p">:</span> <span class="n">dict</span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">custom_weights_loaders</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>transformers.adapters.loading.WeightsLoader<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.save_all_adapters" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves all adapters of this model together with their configuration to subfolders of the given location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>save_directory</strong> (<em>str</em>) – Path to a directory where the adapters should be saved.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.set_active_adapters">
<code class="sig-name descname">set_active_adapters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adapter_setup</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>list<span class="p">, </span>transformers.adapters.composition.AdapterCompositionBlock<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">skip_layers</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.set_active_adapters" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the adapter modules to be used by default in every forward pass. This setting can be overriden by passing
the <cite>adapter_names</cite> parameter in the <cite>foward()</cite> pass. If no adapter with the given name is found, no module of
the respective type will be activated. In case the calling model class supports named prediction heads, this
method will attempt to activate a prediction head with the name of the last adapter in the list of passed
adapter names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>adapter_setup</strong> (<em>list</em>) – The list of adapters to be activated by default. Can be a fusion or stacking configuration.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.set_adapter_fusion_config">
<code class="sig-name descname">set_adapter_fusion_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adapter_fusion_config</span></em>, <em class="sig-param"><span class="n">override_kwargs</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.set_adapter_fusion_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the adapter fusion configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>adapter_fusion_config</strong> (<em>str</em><em> or </em><em>dict</em>) – <p>adapter fusion configuration, can be either:</p>
<ul class="simple">
<li><p>a string identifying a pre-defined adapter fusion configuration</p></li>
<li><p>a dictionary representing the adapter fusion configuration</p></li>
<li><p>the path to a file containing the adapter fusion configuration</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.train_adapter">
<code class="sig-name descname">train_adapter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adapter_setup</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>list<span class="p">, </span>transformers.adapters.composition.AdapterCompositionBlock<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.train_adapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model into mode for training the given adapters.</p>
</dd></dl>

<dl class="py method">
<dt id="transformers.GPT2ModelWithHeads.train_fusion">
<code class="sig-name descname">train_fusion</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adapter_setup</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>list<span class="p">, </span>transformers.adapters.composition.AdapterCompositionBlock<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">unfreeze_adapters</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ModelWithHeads.train_fusion" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model into mode for training of adapter fusion determined by a list of adapter names.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="gpt2lmheadmodel">
<h2>GPT2LMHeadModel<a class="headerlink" href="#gpt2lmheadmodel" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.GPT2LMHeadModel">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GPT2LMHeadModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2LMHeadModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.GPT2LMHeadModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">past_key_values</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoder_attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2LMHeadModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_ids_length)</span></code>) – <p><code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids_length</span></code> = <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> else
<code class="docutils literal notranslate"><span class="pre">past_key_values[0][0].shape[-2]</span></code> (<code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used, only <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> that do not have their past calculated should be
passed as <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.GPT2Tokenizer" title="transformers.GPT2Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code></a>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code> for
details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Tuple[torch.Tensor]]</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>) – Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> output below). Can be used to speed up sequential decoding. The <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> which
have their past given to this model should not be passed as <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> as they have already been
computed.</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_ids_length)</span></code>, <cite>optional</cite>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul>
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – <p>Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used, optionally only the last <code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_embeds</span></code> have to be input (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code>).</p>
</p></li>
<li><p><strong>use_cache</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – If set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> key value states are returned and can be used to speed up
decoding (see <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code>).</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code> instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code class="docutils literal notranslate"><span class="pre">labels</span> <span class="pre">=</span> <span class="pre">input_ids</span></code> Indices are selected in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code> All labels set to
<code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored (masked), the loss is only computed for labels in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size]</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">CausalLMOutputWithCrossAttentions</span></code> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided) – Language modeling loss (for next-token prediction).</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li><p><strong>cross_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tuple(torch.FloatTensor))</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">use_cache=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.use_cache=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> tuples of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>, with each tuple containing the
cached key, value states of the self-attention and the cross-attention layers if model is used in
encoder-decoder setting. Only relevant if <code class="docutils literal notranslate"><span class="pre">config.is_decoder</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> input) to speed up sequential decoding.</p>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">CausalLMOutputWithCrossAttentions</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="gpt2doubleheadsmodel">
<h2>GPT2DoubleHeadsModel<a class="headerlink" href="#gpt2doubleheadsmodel" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.GPT2DoubleHeadsModel">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GPT2DoubleHeadsModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2DoubleHeadsModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The GPT2 Model transformer with a language modeling and a multiple-choice classification head on top e.g. for
RocStories/SWAG tasks. The two heads are two linear layers. The language modeling head has its weights tied to the
input embeddings, the classification head takes as input the input of a specified classification token index in the
input sequence).</p>
<blockquote>
<div><p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="simple">
<dt>config (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>): Model configuration class with all the parameters of the model.</dt><dd><p>Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<dl class="py method">
<dt id="transformers.GPT2DoubleHeadsModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">past_key_values</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mc_token_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mc_labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2DoubleHeadsModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.GPT2DoubleHeadsModel" title="transformers.GPT2DoubleHeadsModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2DoubleHeadsModel</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_ids_length)</span></code>) – <p><code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids_length</span></code> = <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> else
<code class="docutils literal notranslate"><span class="pre">past_key_values[0][0].shape[-2]</span></code> (<code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used, only <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> that do not have their past calculated should be
passed as <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.GPT2Tokenizer" title="transformers.GPT2Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code></a>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code> for
details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Tuple[torch.Tensor]]</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>) – Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> output below). Can be used to speed up sequential decoding. The <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> which
have their past given to this model should not be passed as <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> as they have already been
computed.</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_ids_length)</span></code>, <cite>optional</cite>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul>
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – <p>Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used, optionally only the last <code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_embeds</span></code> have to be input (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code>).</p>
</p></li>
<li><p><strong>use_cache</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – If set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> key value states are returned and can be used to speed up
decoding (see <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code>).</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code> instead of a plain tuple.</p></li>
<li><p><strong>mc_token_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_choices)</span></code>, <cite>optional</cite>, default to index of the last token of the input) – Index of the classification token in each input sequence. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">input_ids.size(-1)</span> <span class="pre">-</span>
<span class="pre">1[</span></code>.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code class="docutils literal notranslate"><span class="pre">labels</span> <span class="pre">=</span> <span class="pre">input_ids</span></code> Indices are selected in <code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size</span> <span class="pre">-</span> <span class="pre">1]</span></code> All labels set to
<code class="docutils literal notranslate"><span class="pre">-100</span></code> are ignored (masked), the loss is only computed for labels in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.vocab_size</span> <span class="pre">-</span> <span class="pre">1]</span></code></p></li>
<li><p><strong>mc_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size)</span></code>, <cite>optional</cite>) – Labels for computing the multiple choice classification loss. Indices should be in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span>
<span class="pre">num_choices]</span></code> where <cite>num_choices</cite> is the size of the second dimension of the input tensors. (see
<cite>input_ids</cite> above)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="#transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput" title="transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2DoubleHeadsModelOutput</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">labels</span></code> is provided) – Language modeling loss.</p></li>
<li><p><strong>mc_loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">mc_labels</span></code> is provided) – Multiple choice classification loss.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_choices,</span> <span class="pre">sequence_length,</span> <span class="pre">config.vocab_size)</span></code>) – Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p></li>
<li><p><strong>mc_logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_choices)</span></code>) – Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).</p></li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Tuple[torch.Tensor]]</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">use_cache=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.use_cache=True</span></code>) – Tuple of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>, containing tuples of tensors of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">embed_size_per_head)</span></code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> input) to speed up sequential decoding.</p>
</li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2DoubleHeadsModel</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">GPT2DoubleHeadsModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Add a [CLS] to the vocabulary (we should train it also!)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_added_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">({</span><span class="s1">&#39;cls_token&#39;</span><span class="p">:</span> <span class="s1">&#39;[CLS]&#39;</span><span class="p">})</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>  <span class="c1"># Update the model embeddings with the new vocabulary size</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">choices</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Hello, my dog is cute [CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello, my cat is cute [CLS]&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_choices</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">choices</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cls_token_location</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokens</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">encoded_choices</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded_choices</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size: 1, number of choices: 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mc_token_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">cls_token_location</span><span class="p">])</span>  <span class="c1"># Batch size: 1</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">mc_token_ids</span><span class="o">=</span><span class="n">mc_token_ids</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mc_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">mc_logits</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput" title="transformers.models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2DoubleHeadsModelOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="gpt2forsequenceclassification">
<h2>GPT2ForSequenceClassification<a class="headerlink" href="#gpt2forsequenceclassification" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.GPT2ForSequenceClassification">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GPT2ForSequenceClassification</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ForSequenceClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>The GPT2 Model transformer with a sequence classification head on top (linear layer).</p>
<p><a class="reference internal" href="#transformers.GPT2ForSequenceClassification" title="transformers.GPT2ForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2ForSequenceClassification</span></code></a> uses the last token in order to do the classification, as
other causal models (e.g. GPT-1) do.</p>
<p>Since it does classification on the last token, it requires to know the position of the last token. If a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">pad_token_id</span></code> is defined in the configuration, it finds the last token that is not a padding token in each
row. If no <code class="xref py py-obj docutils literal notranslate"><span class="pre">pad_token_id</span></code> is defined, it simply takes the last value in each row of the batch. Since it cannot
guess the padding tokens when <code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_embeds</span></code> are passed instead of <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code>, it does the same (take
the last value in each row of the batch).</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.GPT2ForSequenceClassification.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">past_key_values</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">position_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">head_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.GPT2ForSequenceClassification.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.GPT2ForSequenceClassification" title="transformers.GPT2ForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2ForSequenceClassification</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_ids_length)</span></code>) – <p><code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids_length</span></code> = <code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> else
<code class="docutils literal notranslate"><span class="pre">past_key_values[0][0].shape[-2]</span></code> (<code class="docutils literal notranslate"><span class="pre">sequence_length</span></code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used, only <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> that do not have their past calculated should be
passed as <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.GPT2Tokenizer" title="transformers.GPT2Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code></a>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code> for
details.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</p></li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Tuple[torch.Tensor]]</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>) – Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> output below). Can be used to speed up sequential decoding. The <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> which
have their past given to this model should not be passed as <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> as they have already been
computed.</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_ids_length)</span></code>, <cite>optional</cite>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul>
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>position_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">config.max_position_embeddings</span> <span class="pre">-</span> <span class="pre">1]</span></code>.</p>
<p><a class="reference external" href="../glossary.html#position-ids">What are position IDs?</a></p>
</p></li>
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_heads,)</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">(num_layers,</span> <span class="pre">num_heads)</span></code>, <cite>optional</cite>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – <p>Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> is used, optionally only the last <code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_embeds</span></code> have to be input (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code>).</p>
</p></li>
<li><p><strong>use_cache</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – If set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> key value states are returned and can be used to speed up
decoding (see <code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code>).</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code> instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>) – Labels for computing the sequence classification/regression loss. Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span>
<span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">SequenceClassifierOutputWithPast</span></code> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code> is provided) – Classification (or regression if config.num_labels==1) loss.</p></li>
<li><p><strong>logits</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.num_labels)</span></code>) – Classification (or regression if config.num_labels==1) scores (before SoftMax).</p></li>
<li><p><strong>past_key_values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tupel(torch.FloatTensor))</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">use_cache=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.use_cache=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code> of length <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.n_layers</span></code>, with each tuple having 2 tensors
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span> <span class="pre">embed_size_per_head)</span></code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code class="xref py py-obj docutils literal notranslate"><span class="pre">past_key_values</span></code> input) to speed up sequential decoding.</p>
</li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">SequenceClassifierOutputWithPast</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2ForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;microsoft/dialogrpt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;microsoft/dialogrpt&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mbart.html" class="btn btn-neutral float-right" title="MBart" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="distilbert.html" class="btn btn-neutral float-left" title="DistilBERT" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Adapter-Hub Team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>